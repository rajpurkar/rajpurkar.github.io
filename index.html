<!DOCTYPE html><!--Homepage Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>Pranav Rajpurkar</title><meta name="description" content="I'm Pranav Rajpurkar, a PhD student in Stanford's Computer Science department. Here I share some of my projects."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/images/me.jpg"><link rel="image_src" type="image/jpeg" href="/images/me.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/bower_components/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/stylesheets/layout.css"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga')
ga('create', 'UA-57891428-1', 'auto')
ga('send', 'pageview')</script><link rel="stylesheet" href="/stylesheets/index.css"></head><body><div class="outer"><div class="sidebar"><div class="cover" id="leftCover"><div class="vertical-center"><div class="container"><div class="row"><img class="media-object img-circle" id="me" src="/images/PranavRajpurkar.jpg"><h1>Pranav Rajpurkar</h1><h3>PhD student in Computer Science <br> Stanford University</h3><h4>pranavsr@cs.stanford.edu</h4><ul class="list-inline outside-links"><li><a href="https://github.com/rajpurkar"><i class="fa fa-github"></i></a></li><li><a href="https://twitter.com/pranavrajpurkar"><i class="fa fa-twitter"></i></a></li><li><a href="http://dblp.uni-trier.de/pers/hd/r/Rajpurkar:Pranav"><i class="fa fa-newspaper-o"></i></a></li></ul></div></div></div></div></div><div class="contentbar"><div class="cover" id="rightCover"><div class="col-lg-8 col-lg-offset-1 col-md-9 col-md-offset-1 col-sm-10 col-sm-offset-1"><h1>About</h1><p> I am a Ph.D. student in the <a href="https://stanfordmlgroup.github.io/">Stanford Machine Learning Group</a> where I'm advised by Professor Andrew Ng and Professor Percy Liang. My research focuses on developing machine learning algorithms and applying AI solutions to high-impact problems.<div class="project"><h2>ChexNet - Radiologist-Level Pneumonia Detection </h2><h3>Active project for 2 months with Jeremy Irvin and Professor Matt Lungren, Professor Andrew Ng</h3><p>We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our model, CheXNet, is a 121-layer convolutional neural network that inputs a chest X-ray image and outputs the probability of pneumonia along with a heatmap localizing the areas of the image most indicative of pneumonia. We train on ChestX-ray14, the largest publicly available chest X-ray dataset. We find that the model exceeds the average radiologist performance at the pneumonia detection task on both sensitivity and specificity.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/projects/chexnet">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://arxiv.org/abs/1711.05225">Paper (Arxiv)</a></li><li><a class="btn actionBtn inverseBtn" href="https://news.stanford.edu/2017/11/15/algorithm-outperforms-radiologists-diagnosing-pneumonia/">Press (Stanford News)</a></li><li><a class="btn actionBtn inverseBtn" href="https://spectrum.ieee.org/the-human-os/biomedical/diagnostics/stanford-algorithm-can-diagnose-pneumonia-better-than-radiologists">Press (IEEE Spectrum)</a></li></ul></div><div class="project"><h2>Cardiologist-Level Arrhythmia Detection </h2><h3>Active project for a year with Awni Hannun and Professor Andrew Ng</h3><p>Our deep learning algorithm exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. Dataset 500x larger than previously studied corpora used to train a deep convolutional neural network.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/projects/ecg">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://arxiv.org/abs/1707.01836">Paper (Arxiv)</a></li><li><a class="btn actionBtn inverseBtn" href="http://news.stanford.edu/2017/07/06/algorithm-diagnoses-heart-arrhythmias-cardiologist-level-accuracy/">Press (Stanford News)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.technologyreview.com/s/608234/the-machines-are-getting-ready-to-play-doctor/">Press (MIT Technology Review)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.youtube.com/watch?v=XVDDEsmbjuE">Video (Stanford)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.youtube.com/watch?v=_sh8FkKmKI8">Podcast (Data Skeptic)</a></li></ul></div><div class="project"><h2>SQuAD </h2><h3>Active project for 2 years with Professor Percy Liang</h3><p>Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 107,785 question-answer pairs on 536 articles, SQuAD is significantly larger than previous reading comprehension datasets.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanford-qa.com">SQuAD</a></li><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1606.05250">Paper (EMNLP 2016) (Best Resource Paper)</a></li><li><a class="btn actionBtn inverseBtn" href="https://rajpurkar.github.io/mlx/qa-and-squad/">Blogpost</a></li></ul></div><div class="project"><h2>Edusalsa </h2><h3>Active project for 4 years with Brad Girardeau</h3><p>A single class can transform a life. A popular introduction to programming class leads to the discovery of a passion for computer science, a social dance class exposes a deep appreciation for artistic expression--experiences like these are at the core of a Stanford education. Yet out of the 5000 classes offered here, students only have time to take less than 1% during their undergraduate career. This small selection of classes determines the foundation on which passions are developed - passions that lead to great innovations and great discoveries that change the world. Some students arrive at Stanford with clear visions of their futures. Others need a little time to explore and decide what to do with their lives. Edusalsa lets students find the classes where they can discover their passions, equipping them with new tools on their path of intellectual discovery, infusing life and vitality into the Stanford experience.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://edusalsa.com">Edusalsa</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.stanforddaily.com/tag/edusalsa/">Press (Stanford Daily)</a></li></ul></div><div class="project"><h2>Driverseat </h2><h3>From Jan 2013 up to May 2015 with Professor Andrew Ng</h3><p>Research in Autonomous Driving spanning Computer Vision, Artificial Intelligence, and Crowdsourcing. My undergraduate honors research introduced Driverseat, a technology for embedding crowds around learning systems for autonomous driving.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1512.01872">Paper 1 (CrowdML@ICML'15)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.technologyreview.com/s/544926/ai-machine-learns-to-drive-using-crowdteaching/">Press (MIT Technology Review)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.youtube.com/watch?v=7b3XEBVGQHs">Driverseat video demo</a></li><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1504.01716">Paper 2 (Arxiv)</a></li></ul></div><div class="project"><h2>Recommend Papers </h2><h3>From Jul 2015 up to Dec 2015 with Professor Andrew Ng, Professor Yoshua Bengio et al.</h3><p>Piloted for the Deep Learning Symposium at NIPS '15, Recommend-Papers was built in order to facilitate discussion of the most recent deep learning breakthroughs and explore an alternative mechanism for selecting presentations. In order to let the broader research community (including the authors of research papers) contribute to the discussion, Recommend-Papers allowed members to post papers and comment on them, and PC members to hold private discussions.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://Recommend-Papers.org">Recommend-Papers.org</a></li></ul></div><div class="project"><h2>Chord Recognition </h2><h3>From Oct 2014 up to Dec 2014 with Brad Girardeau and Toki Migimatsu</h3><p>Can a computer identify the chord I'm playing on a guitar simply by  listening to it? How well does machine learning perform on the task realtime? Could we leverage that technology to give realtime feedback to an instrument learner? This research presents a prototype of an online tool for real-time chord recognition. It fuses traditional techniques in machine learning with the capabilities of new web technologies such the the Web Audio API, and WebSockets.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="/files/GirardeauMigimatsuRajpurkar-ASupervisedApproachToChordRecognitionFinal.pdf">Paper (SURJ '15)</a></li></ul></div><div class="project"><h2>MLX </h2><h3>Active project for a year</h3><p>Machine Learning Experiments (mlx) is a blog to showcase machine learning work intended to showcase machine learning experiments not just in their final polished form, but also highlight the thought process that guides research.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://rajpurkar.github.io/mlx/">MLX Blog</a></li></ul></div><div class="project"><h2>Augur </h2><h3>From Jun 2014 up to Sep 2014 with Ethan Fast and Professor Michael Bernstein</h3><p>Research in Human Computer Interaction, and Natural Language Processing, exploring how we could teach a computer enough about human actions to enable predictive application interfaces that could, for example, recommend ice cream shops upon learning that a person was having dinner.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1602.06977">Paper (CHI 2016) (Best Paper Honorable Mention)</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.theglobeandmail.com/technology/tech-news/stanford-researchers-using-wattpad-stories-to-inform-artificial-intelligence/article29042748/">Press (The Globe and Mail)</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.engadget.com/2016/02/28/ai-learns-to-predict-human-reactions-by-reading-our-fiction/">Press (Engadget)</a></li><li><a class="btn actionBtn inverseBtn" href="https://thestack.com/cloud/2016/02/26/computers-read-1-8-billion-words-of-fiction-to-learn-how-to-anticipate-human-behaviour/">Press (The Stack)</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.teleread.com/can-books-teach-machines-to-understand-people/">Press (Teleread)</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.hngn.com/articles/183248/20160229/trained-understand-predict-human-behavior-reading-books.htm">Press (HNGN)</a></li><li><a class="btn actionBtn inverseBtn" href="http://dl.acm.org/citation.cfm?id=2732805">Paper (CHI EA '15)</a></li></ul></div><div class="project"><h2>Vocalet </h2><h3>From Jan 2013 up to Mar 2013 with Vincent Su</h3><p>Singing is awesome, powerful, and personal. Can we simplify, for amateur singers, the process of exploring new songs to sing along to? Vocalet provides a simple interface for singing enthusiasts to enjoy. It's easy to sing along to karaoke versions of songs, and get inspired by cover artists.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://vocalet.com">Vocalet</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.youtube.com/embed/LPoLC1uTH9w">Showcase Video</a></li></ul></div><div class="project"><h2>Stanford Spaghetti </h2><h3>Active project for 2 years with Gili Rusak</h3><p>Stanford spaghetti creates opportunities for students to meet other students around campus.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordspaghetti.com">Stanford Spaghetti</a></li></ul></div></p></div></div></div></div><script src="/bower_components/jquery/dist/jquery.min.js"></script><script src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>