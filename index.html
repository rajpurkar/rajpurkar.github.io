<!DOCTYPE html><!--Homepage Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>Pranav Rajpurkar</title><meta name="description" content="I'm Pranav Rajpurkar, a PhD student in Stanford's Computer Science department. Here I share some of my projects."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/images/me.jpg"><link rel="image_src" type="image/jpeg" href="/images/me.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/bower_components/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/stylesheets/layout.css"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga')
ga('create', 'UA-57891428-1', 'auto')
ga('send', 'pageview')</script><link rel="stylesheet" href="/stylesheets/index.css"></head><body><div class="outer"><div class="sidebar"><div class="cover" id="leftCover"><div class="vertical-center"><div class="container"><div class="row"><img class="media-object img-circle" id="me" src="/images/PranavRajpurkar.jpg"><h1>Pranav Rajpurkar</h1><h3>PhD student in Computer Science <br> Stanford University</h3><h4>pranavsr@cs.stanford.edu</h4><ul class="list-inline outside-links"><li><a href="https://twitter.com/pranavrajpurkar"><i class="fa fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=QcOG6sgAAAAJ"><i class="fa fa-mortar-board"></i></a></li><li><a href="https://github.com/rajpurkar"><i class="fa fa-github"></i></a></li></ul></div></div></div></div></div><div class="contentbar"><div class="cover" id="rightCover"><div class="col-lg-8 col-lg-offset-1 col-md-9 col-md-offset-1 col-sm-10 col-sm-offset-1"><h2>Bio</h2><p> I’m a 4th year PhD candidate in the Stanford Machine Learning Group co-advised by Professor Andrew Ng and Professor Percy Liang. I work on the development and deployment of deep learning algorithms for automated diagnosis, prognosis, and treatment of diseases. I have developed models for automated detection of arrhythmias, multiple pathology detection under uncertainty for x-rays (CheXNet, MURA, CheXNeXt), and augmentation of experts in knee MRI interpretation (MRNet). I have also developed SQuAD, a machine reading comprehension dataset.<div class="project"><h2>MRNet - Deep-learning-assisted diagnosis for knee MRI </h2><h3>From Dec 2017 up to Nov 2018 with Nicholas Bien and Professor Matt Lungren, Professor Andrew Ng</h3><p>We developed an algorithm to predict abnormalities in knee MRI exams, and measured the clinical utility of providing the algorithm’s predictions to radiologists and surgeons during interpretation. 7 practicing board-certified general radiologists and 2 practicing orthopedic surgeons at Stanford University Medical Center (3–29 years in practice, average 12 years) read a validation set of 120 exams twice, once without model assistance and once with model assistance, separated by a washout period of at least 10 days. We found that model assistance significantly reduced the rate at which healthy patients would be mistakenly diagnosed as having ACL tears.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/projects/mrnet">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002699">Paper (PLOS Medicine)</a></li><li><a class="btn actionBtn inverseBtn" href="https://scopeblog.stanford.edu/2018/11/27/ai-doctors-team-up-to-improve-expedite-diagnoses/">Press (Stanford SCOPE Blog)</a></li></ul><div class="images row"><div class="col-sm-12"> <img src="https://stanfordmlgroup.github.io/projects/mrnet/img/fig6.png"/></div></div></div><div class="project"><h2>CheXNeXt - Deep learning for chest radiograph diagnosis </h2><h3>From Dec 2017 up to Nov 2018 with Jeremy Irvin and Professor Matt Lungren, Professor Andrew Ng</h3><p>We developed CheXNeXt, a deep learning algorithm to concurrently detect 14 clinically important diseases in chest radiographs. CheXNeXt's training process consists of 2 consecutive stages to account for the partially incorrect labels in the ChestX-ray14 dataset. We evaluated the algorithm against 9 practicing radiologists on a validation set of 420 images for which the majority vote of 3 cardiothoracic specialty radiologists served as ground truth. The algorithm achieved performance equivalent to the practicing radiologists on 10 pathologies, better on 1 pathology, and worse on 3 pathologies.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/projects/chexnext">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686">Paper (PLOS Medicine)</a></li></ul><div class="images row"><div class="col-sm-12"> <img src="https://stanfordmlgroup.github.io/projects/chexnext/img/point-compares.png"/></div></div><div class="videos row"><div class="col-sm-12"> <iframe width="100%" height="400px" src="https://youtube.com/embed/VJRCj-4E2iU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="allowfullscreen"></iframe></div></div></div><div class="project"><h2>MURA - Large Dataset for Abnormality Detection in Bone X-Rays </h2><h3>From Sep 2017 up to Apr 2018 with Jeremy Irvin and Professor Matt Lungren, Professor Andrew Ng</h3><p>MURA is a large dataset of bone X-rays. Algorithms are tasked with determining whether an X-ray study is normal or abnormal. Musculoskeletal conditions affect more than 1.7 billion people worldwide, and are the most common cause of severe, long-term pain and disability, with 30 million emergency department visits annually and increasing. We hope that our dataset can lead to significant advances in medical imaging technologies which can diagnose at the level of experts, towards improving healthcare access in parts of the world where access to skilled radiologists is limited.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/competitions/mura">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://arxiv.org/abs/1712.06957">Paper (MIDL Conference)</a></li></ul><div class="images row"><div class="col-sm-12"> <img src="//stanfordmlgroup.github.io/competitions/mura/img/mura-cam1.svg"/></div></div></div><div class="project"><h2>SQuAD2.0 </h2><h3>Active project for 2 years with Robin Jia and Professor Percy Liang</h3><p>SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD2.0 is a challenging natural language understanding task for existing models, and we release SQuAD2.0 to the community as the successor to SQuAD1.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanford-qa.com">SQuAD2.0</a></li><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1806.03822">Paper (ACL 2018) (Best Short Paper)</a></li></ul></div><div class="project"><h2>CheXNet - Radiologist-Level Pneumonia Detection </h2><h3>From Sep 2017 up to Dec 2017 with Jeremy Irvin and Professor Matt Lungren, Professor Andrew Ng</h3><p>We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our model, CheXNet, is a 121-layer convolutional neural network that inputs a chest X-ray image and outputs the probability of pneumonia along with a heatmap localizing the areas of the image most indicative of pneumonia. We train on ChestX-ray14, the largest publicly available chest X-ray dataset. We find that the model exceeds the average radiologist performance at the pneumonia detection task on both sensitivity and specificity.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/projects/chexnet">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://arxiv.org/abs/1711.05225">Paper (Arxiv)</a></li><li><a class="btn actionBtn inverseBtn" href="https://news.stanford.edu/2017/11/15/algorithm-outperforms-radiologists-diagnosing-pneumonia/">Press (Stanford News)</a></li></ul><div class="images row"><div class="col-sm-6"> <img src="//stanfordmlgroup.github.io/projects/chexnet/img/chest-cam.png"/></div><div class="col-sm-6"> <img src="//stanfordmlgroup.github.io/projects/chexnet/img/chest-example.png"/></div></div></div><div class="project"><h2>Cardiologist-Level Arrhythmia Detection </h2><h3>From Jan 2017 up to Jul 2017 with Awni Hannun and Professor Andrew Ng</h3><p>Our deep learning algorithm exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. Dataset 500x larger than previously studied corpora used to train a deep convolutional neural network.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanfordmlgroup.github.io/projects/ecg">Project Webpage</a></li><li><a class="btn actionBtn inverseBtn" href="https://arxiv.org/abs/1707.01836">Paper (Arxiv)</a></li><li><a class="btn actionBtn inverseBtn" href="http://news.stanford.edu/2017/07/06/algorithm-diagnoses-heart-arrhythmias-cardiologist-level-accuracy/">Press (Stanford News)</a></li></ul><div class="images row"><div class="col-sm-6"> <img src="//stanfordmlgroup.github.io/projects/ecg/img/main-figure.svg"/></div><div class="col-sm-6"> <img src="//stanfordmlgroup.github.io/projects/ecg/img/ecg-annotation.svg"/></div></div><div class="videos row"><div class="col-sm-12"> <iframe width="100%" height="400px" src="//www.youtube.com/embed/XVDDEsmbjuE?ecver=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="allowfullscreen"></iframe></div></div></div><div class="project"><h2>SQuAD </h2><h3>From Jan 2016 up to Dec 2016 with Professor Percy Liang</h3><p>Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 107,785 question-answer pairs on 536 articles, SQuAD is significantly larger than previous reading comprehension datasets.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://stanford-qa.com">SQuAD</a></li><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1606.05250">Paper (EMNLP 2016) (Best Resource Paper)</a></li><li><a class="btn actionBtn inverseBtn" href="https://rajpurkar.github.io/mlx/qa-and-squad/">Blogpost</a></li></ul></div><div class="project"><h2>Edusalsa </h2><h3>Active project for 5 years with Brad Girardeau</h3><p>A single class can transform a life. A popular introduction to programming class leads to the discovery of a passion for computer science, a social dance class exposes a deep appreciation for artistic expression--experiences like these are at the core of a Stanford education. Yet out of the 5000 classes offered here, students only have time to take less than 1% during their undergraduate career. This small selection of classes determines the foundation on which passions are developed - passions that lead to great innovations and great discoveries that change the world. Some students arrive at Stanford with clear visions of their futures. Others need a little time to explore and decide what to do with their lives. Edusalsa lets students find the classes where they can discover their passions, equipping them with new tools on their path of intellectual discovery, infusing life and vitality into the Stanford experience.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://edusalsa.com">Edusalsa</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.stanforddaily.com/tag/edusalsa/">Press (Stanford Daily)</a></li></ul></div><div class="project"><h2>Driverseat </h2><h3>From Jan 2013 up to May 2015 with Professor Andrew Ng</h3><p>Research in Autonomous Driving spanning Computer Vision, Artificial Intelligence, and Crowdsourcing. My undergraduate honors research introduced Driverseat, a technology for embedding crowds around learning systems for autonomous driving.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1512.01872">Paper 1 (CrowdML@ICML'15)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.technologyreview.com/s/544926/ai-machine-learns-to-drive-using-crowdteaching/">Press (MIT Technology Review)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.youtube.com/watch?v=7b3XEBVGQHs">Driverseat video demo</a></li><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1504.01716">Paper 2 (Arxiv)</a></li></ul><div class="images row"><div class="col-sm-12"> <img src="//cdn.technologyreview.com/i/images/Driverseat.PNG"/></div></div></div><div class="project"><h2>Recommend Papers </h2><h3>From Jul 2015 up to Dec 2015 with Professor Andrew Ng, Professor Yoshua Bengio et al.</h3><p>Piloted for the Deep Learning Symposium at NIPS '15, Recommend-Papers was built in order to facilitate discussion of the most recent deep learning breakthroughs and explore an alternative mechanism for selecting presentations. In order to let the broader research community (including the authors of research papers) contribute to the discussion, Recommend-Papers allowed members to post papers and comment on them, and PC members to hold private discussions.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://recommend-papers.herokuapp.com/">Recommend-Papers</a></li></ul></div><div class="project"><h2>Chord Recognition </h2><h3>From Oct 2014 up to Dec 2014 with Brad Girardeau and Toki Migimatsu</h3><p>Can a computer identify the chord I'm playing on a guitar simply by  listening to it? How well does machine learning perform on the task realtime? Could we leverage that technology to give realtime feedback to an instrument learner? This research presents a prototype of an online tool for real-time chord recognition. It fuses traditional techniques in machine learning with the capabilities of new web technologies such the the Web Audio API, and WebSockets.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="/files/GirardeauMigimatsuRajpurkar-ASupervisedApproachToChordRecognitionFinal.pdf">Paper (SURJ '15)</a></li></ul></div><div class="project"><h2>MLX </h2><h3>Active project for 2 years</h3><p>Machine Learning Experiments (mlx) is a blog to showcase machine learning work intended to showcase machine learning experiments not just in their final polished form, but also highlight the thought process that guides research.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://rajpurkar.github.io/mlx/">MLX Blog</a></li></ul></div><div class="project"><h2>Augur </h2><h3>From Jun 2014 up to Sep 2014 with Ethan Fast and Professor Michael Bernstein</h3><p>Research in Human Computer Interaction, and Natural Language Processing, exploring how we could teach a computer enough about human actions to enable predictive application interfaces that could, for example, recommend ice cream shops upon learning that a person was having dinner.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1602.06977">Paper (CHI 2016) (Best Paper Honorable Mention)</a></li><li><a class="btn actionBtn inverseBtn" href="http://dl.acm.org/citation.cfm?id=2732805">Paper (CHI EA '15)</a></li></ul></div><div class="project"><h2>Vocalet </h2><h3>From Jan 2013 up to Mar 2013 with Vincent Su</h3><p>Singing is awesome, powerful, and personal. Can we simplify, for amateur singers, the process of exploring new songs to sing along to? Vocalet provides a simple interface for singing enthusiasts to enjoy. It's easy to sing along to karaoke versions of songs, and get inspired by cover artists.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://vocalet.com">Vocalet</a></li><li><a class="btn actionBtn inverseBtn" href="http://www.youtube.com/embed/LPoLC1uTH9w">Showcase Video</a></li></ul></div></p></div></div></div></div><script src="/bower_components/jquery/dist/jquery.min.js"></script><script src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>